{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/LuthandoMaqondo/phenaki-pytorch/blob/luthando-contribution/notebooks/training.ipynb\" target=\"_blank\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mount the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import platform\n",
    "import requests\n",
    "import torch\n",
    "from torch.utils.data import Dataset, ConcatDataset, DataLoader\n",
    "import imageio\n",
    "from matplotlib import pyplot as plt, animation\n",
    "from IPython.display import display, Image, HTML\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    WORKING_DIR = '.'\n",
    "    IN_COLAB = False\n",
    "if IN_COLAB:\n",
    "    WORKING_DIR = '/content/drive/MyDrive/Colab Notebooks'\n",
    "    drive.mount('/content/drive',  force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_built() else \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_COLAB:\n",
    "    !git clone https://github.com/LuthandoMaqondo/phenaki-pytorch.git\n",
    "    %cd /content/phenaki-pytorch\n",
    "    !git checkout luthando-contribution\n",
    "    !pip install -r requirements.txt\n",
    "    # !pip install phenaki-pytorch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autovisual import DatasetConfig, VideoCustomDataset\n",
    "# dataset_config_args = {\n",
    "#     'refreshData': True,\n",
    "#     # 'useCLIP': \"openai/clip-vit-large-patch14\", # Use the pretrained CLIP model for handling ALL Text inputs.\n",
    "#     'structure': 'text_video_pair',\n",
    "#     'tokenize_text': False,\n",
    "#     # 'data_folder': f'.{WORKING_DIR}/datasets/Appimate',\n",
    "#     'data_folder': 'https://appimate1storage.blob.core.windows.net/datasets/Appimate',\n",
    "#     'data_json': \"dataset.json\",\n",
    "#     'data_points': None, # None\n",
    "\n",
    "#     'max_text_length': 77,\n",
    "#     'max_num_frames': 6,\n",
    "#     'resolution': 256,\n",
    "#     'num_channels': 1, \n",
    "#     'normalize': True,\n",
    "#     'scale_to': 0,#0.5,\n",
    "#     'has_start_end_token': True,\n",
    "\n",
    "#     'frame_rate': 2,\n",
    "#     'frame_rate_ratio': 0.01,\n",
    "#     'output_format': 'TCHW'\n",
    "# }\n",
    "# datasetConfig = DatasetConfig(**dataset_config_args, train=True)\n",
    "# custom_dataset = VideoCustomDataset(datasetConfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MockTextVideoDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        length = 100,\n",
    "        image_size = 256,\n",
    "        num_frames = 17\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_frames = num_frames\n",
    "        self.image_size = image_size\n",
    "        self.len = length\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx): # Video data of shape: CTHW\n",
    "        video = torch.randn(3, self.num_frames, self.image_size, self.image_size)\n",
    "        caption = f'video caption {idx}'\n",
    "        return video, caption\n",
    "\n",
    "mock_dataset = MockTextVideoDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = ConcatDataset([\n",
    "    mock_dataset,\n",
    "    # custom_dataset\n",
    "])\n",
    "\n",
    "# train_len = int(len(full_dataset) * (datasetConfig.train_split) )\n",
    "train_len = int(len(full_dataset) * (.9) )\n",
    "train_dataset, eval_dataset = torch.utils.data.random_split(full_dataset, [train_len, len(full_dataset)- train_len])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phenaki (Components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from phenaki_pytorch import CViViT, CViViTTrainer, MaskGit, Phenaki, PhenakiTrainer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Causal-ViViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/luthando/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with dataset of 4768 samples and validating with randomly splitted 251 samples\n"
     ]
    }
   ],
   "source": [
    "cvivit = CViViT(\n",
    "    dim = 512,\n",
    "    codebook_size = 65536,\n",
    "    image_size = (256, 256),\n",
    "    patch_size = 32,\n",
    "    temporal_patch_size = 2,\n",
    "    spatial_depth = 4,\n",
    "    temporal_depth = 4,\n",
    "    dim_head = 64,\n",
    "    heads = 8\n",
    ").to(device)\n",
    "\n",
    "data_folder = os.path.expanduser(f\"~/.cache/datasets\")\n",
    "trainer = CViViTTrainer(\n",
    "    cvivit,\n",
    "    folder = data_folder,\n",
    "    batch_size = 4,\n",
    "    grad_accum_every = 4,\n",
    "    train_on_images = False,  # you can train on images first, before fine tuning on video, for sample efficiency\n",
    "    use_ema = True,          # recommended to be turned on (keeps exponential moving averaged cvivit) unless if you don't have enough resources\n",
    "\n",
    "    save_results_every = 100,\n",
    "    save_model_every = 100,\n",
    "    num_train_steps = 10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04/03/2024 at 00:41:29\n",
      "0: vae loss: 24695.996826171875 - discr loss: 12.016433715820312\n",
      "0: saving to results\n",
      "0: saving model to results\n",
      "1: vae loss: 20232.353271484375 - discr loss: 10.936233758926392\n",
      "2: vae loss: 17431.91796875 - discr loss: 10.75072431564331\n",
      "3: vae loss: 17244.566650390625 - discr loss: 10.229156017303467\n",
      "4: vae loss: 22994.951171875 - discr loss: 7.5924235582351685\n",
      "5: vae loss: 18668.1982421875 - discr loss: 0.5987280756235123\n",
      "6: vae loss: 17323.72412109375 - discr loss: 1443.1932678222656\n",
      "7: vae loss: 22257.77197265625 - discr loss: 0.49602970853447914\n",
      "8: vae loss: 21792.02587890625 - discr loss: 1.414475828409195\n",
      "9: vae loss: 20138.2255859375 - discr loss: 4.482433319091797\n",
      "10: vae loss: 15859.2939453125 - discr loss: 4.648132503032684\n",
      "11: vae loss: 26069.21044921875 - discr loss: 4.876460671424866\n",
      "12: vae loss: 13698.11669921875 - discr loss: 3.7602545022964478\n",
      "13: vae loss: 20418.930908203125 - discr loss: 3.162919044494629\n",
      "14: vae loss: 13798.666259765625 - discr loss: 1.4945070445537567\n",
      "15: vae loss: 20057.1650390625 - discr loss: 1.1075562834739685\n",
      "16: vae loss: 16362.715087890625 - discr loss: 1.5367950946092606\n",
      "17: vae loss: 14600.295654296875 - discr loss: 6.916336536407471\n",
      "18: vae loss: 19433.239501953125 - discr loss: 10.247456550598145\n",
      "19: vae loss: 13457.138916015625 - discr loss: 9.073500514030457\n",
      "20: vae loss: 17945.3251953125 - discr loss: 9.60564112663269\n",
      "21: vae loss: 14680.802490234375 - discr loss: 10.089131832122803\n",
      "22: vae loss: 12317.490600585938 - discr loss: 9.108011245727539\n",
      "23: vae loss: 16026.77685546875 - discr loss: 10.768033981323242\n",
      "24: vae loss: 21395.659912109375 - discr loss: 5.090029001235962\n",
      "25: vae loss: 16360.17431640625 - discr loss: 7.309955477714539\n",
      "26: vae loss: 17912.26220703125 - discr loss: 8.940979242324829\n",
      "27: vae loss: 16160.917236328125 - discr loss: 11.101028680801392\n",
      "28: vae loss: 20937.57080078125 - discr loss: 11.14067554473877\n",
      "29: vae loss: 18771.498779296875 - discr loss: 11.108357191085815\n",
      "30: vae loss: 16290.119384765625 - discr loss: 10.849489450454712\n",
      "31: vae loss: 15512.2958984375 - discr loss: 11.137789011001587\n",
      "32: vae loss: 15927.93896484375 - discr loss: 11.205324649810791\n",
      "33: vae loss: 15970.848388671875 - discr loss: 11.208486795425415\n",
      "34: vae loss: 19569.311767578125 - discr loss: 11.16921329498291\n",
      "35: vae loss: 13295.461181640625 - discr loss: 11.166740417480469\n",
      "36: vae loss: 14045.364013671875 - discr loss: 10.929174184799194\n",
      "37: vae loss: 12816.245849609375 - discr loss: 10.620808124542236\n",
      "38: vae loss: 17025.70068359375 - discr loss: 9.663964033126831\n",
      "39: vae loss: 15664.675048828125 - discr loss: 8.917459964752197\n",
      "40: vae loss: 18986.79248046875 - discr loss: 7.979064345359802\n",
      "41: vae loss: 18847.69189453125 - discr loss: 7.045992374420166\n",
      "42: vae loss: 17159.04248046875 - discr loss: 7.89231538772583\n",
      "43: vae loss: 15752.52099609375 - discr loss: 8.149691104888916\n",
      "44: vae loss: 16815.56591796875 - discr loss: 68.56488418579102\n",
      "45: vae loss: 17477.408203125 - discr loss: 11.305111408233643\n",
      "46: vae loss: 14485.88037109375 - discr loss: 8.807271480560303\n",
      "47: vae loss: 14453.254150390625 - discr loss: 11.1494722366333\n",
      "48: vae loss: 16061.747802734375 - discr loss: 11.416733264923096\n",
      "49: vae loss: 17038.195068359375 - discr loss: 11.668006658554077\n",
      "50: vae loss: 19439.601806640625 - discr loss: 11.76711106300354\n",
      "51: vae loss: 9330.925903320312 - discr loss: 11.503354549407959\n",
      "52: vae loss: 21242.858154296875 - discr loss: 11.436709642410278\n",
      "53: vae loss: 14857.1962890625 - discr loss: 11.586077451705933\n",
      "54: vae loss: 21435.228515625 - discr loss: 11.626410484313965\n",
      "55: vae loss: 13586.813354492188 - discr loss: 11.425585508346558\n",
      "56: vae loss: 13298.866455078125 - discr loss: 12.781140327453613\n",
      "57: vae loss: 21777.7802734375 - discr loss: 13.053083658218384\n",
      "58: vae loss: 23157.1865234375 - discr loss: 14.290771722793579\n",
      "59: vae loss: 15457.737548828125 - discr loss: 13.92624306678772\n",
      "60: vae loss: 16784.5107421875 - discr loss: 12.764931678771973\n",
      "61: vae loss: 15950.764892578125 - discr loss: 11.784562587738037\n",
      "62: vae loss: 16229.8212890625 - discr loss: 11.147305965423584\n",
      "63: vae loss: 13011.736694335938 - discr loss: 11.25987958908081\n",
      "64: vae loss: 18108.231689453125 - discr loss: 11.16397500038147\n",
      "65: vae loss: 18856.59814453125 - discr loss: 10.798551797866821\n",
      "66: vae loss: 16442.98779296875 - discr loss: 10.856171369552612\n",
      "67: vae loss: 22234.451171875 - discr loss: 10.810032606124878\n",
      "68: vae loss: 13437.6162109375 - discr loss: 10.923563003540039\n",
      "69: vae loss: 13682.240966796875 - discr loss: 11.154057741165161\n",
      "70: vae loss: 19367.748046875 - discr loss: 10.421942472457886\n",
      "71: vae loss: 18181.47705078125 - discr loss: 10.587718725204468\n",
      "72: vae loss: 17669.212280273438 - discr loss: 10.556563377380371\n",
      "73: vae loss: 15715.545654296875 - discr loss: 10.286961555480957\n",
      "74: vae loss: 18321.666748046875 - discr loss: 9.84134030342102\n",
      "75: vae loss: 23470.7666015625 - discr loss: 9.531067132949829\n",
      "76: vae loss: 15819.292236328125 - discr loss: 9.293474674224854\n",
      "77: vae loss: 17247.18408203125 - discr loss: 7.997655034065247\n",
      "78: vae loss: 21593.46826171875 - discr loss: 7.906136751174927\n",
      "79: vae loss: 17485.5322265625 - discr loss: 4.742981433868408\n",
      "80: vae loss: 14077.912963867188 - discr loss: 1.0288507640361786\n",
      "81: vae loss: 14312.1240234375 - discr loss: 8.938838481903076\n",
      "82: vae loss: 22883.7763671875 - discr loss: 84.87860679626465\n",
      "83: vae loss: 18186.145751953125 - discr loss: 12.35454249382019\n",
      "84: vae loss: 19303.154296875 - discr loss: 16.608829975128174\n",
      "85: vae loss: 20101.11865234375 - discr loss: 16.770214557647705\n",
      "86: vae loss: 18145.03466796875 - discr loss: 12.555779218673706\n",
      "87: vae loss: 21816.59716796875 - discr loss: 11.547818183898926\n",
      "88: vae loss: 15971.00146484375 - discr loss: 12.544439315795898\n",
      "89: vae loss: 11041.8408203125 - discr loss: 12.865728378295898\n",
      "90: vae loss: 15694.714111328125 - discr loss: 14.1613028049469\n",
      "91: vae loss: 11383.678100585938 - discr loss: 13.878294229507446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mov,mp4,m4a,3gp,3g2,mj2 @ 0x55c0a467e040] moov atom not found\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m               \u001b[38;5;66;03m# reconstructions and checkpoints will be saved periodically to ./results\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/phenaki-pytorch/phenaki_pytorch/cvivit_trainer.py:337\u001b[0m, in \u001b[0;36mCViViTTrainer.train\u001b[0;34m(self, log_fn)\u001b[0m\n\u001b[1;32m    335\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvae\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    336\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_train_steps:\n\u001b[0;32m--> 337\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m     log_fn(logs)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining complete\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/phenaki-pytorch/phenaki_pytorch/cvivit_trainer.py:230\u001b[0m, in \u001b[0;36mCViViTTrainer.train_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;66;03m# update vae (generator)\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrad_accum_every):\n\u001b[0;32m--> 230\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdl_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mautocast():\n",
      "File \u001b[0;32m~/Desktop/phenaki-pytorch/phenaki_pytorch/cvivit_trainer.py:38\u001b[0m, in \u001b[0;36mcycle\u001b[0;34m(dl)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcycle\u001b[39m(dl):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdl\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/accelerate/data_loader.py:462\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    461\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 462\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-gpu/lib/python3.12/site-packages/torch/utils/data/dataset.py:399\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 399\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m~/Desktop/phenaki-pytorch/phenaki_pytorch/data.py:237\u001b[0m, in \u001b[0;36mVideoDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    235\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgif_to_tensor(path)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 237\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmp4_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munknown extension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/phenaki-pytorch/phenaki_pytorch/data.py:150\u001b[0m, in \u001b[0;36mvideo_to_tensor\u001b[0;34m(path, num_frames, crop_size)\u001b[0m\n\u001b[1;32m    146\u001b[0m         frame \u001b[38;5;241m=\u001b[39m crop_center(frame, \u001b[38;5;241m*\u001b[39mpair(crop_size))\n\u001b[1;32m    148\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(rearrange(frame, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m... -> 1 ...\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m--> 150\u001b[0m frames \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframes\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m)  \u001b[38;5;66;03m# convert list of frames to numpy array\u001b[39;00m\n\u001b[1;32m    151\u001b[0m frames \u001b[38;5;241m=\u001b[39m rearrange(frames, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf h w c -> c f h w\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    153\u001b[0m frames_torch \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(frames)\u001b[38;5;241m.\u001b[39mfloat()\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "trainer.train()               # reconstructions and checkpoints will be saved periodically to ./results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from phenaki_pytorch.data import video_tensor_to_gif\n",
    "# from IPython.display import display, Image\n",
    "# for i, tensor in enumerate(final.unbind(dim = 0)):\n",
    "    \n",
    "#     print('real video:')\n",
    "#     video_tensor_to_gif(real_frames[i].cpu(), 'original_video_'+str(i)+'.gif')\n",
    "#     display(Image('original_video_'+str(i)+'.gif'))\n",
    "    \n",
    "#     print('reconstruction:')\n",
    "#     video_tensor_to_gif(tensor.cpu(), 'reconstructed_video_'+str(i)+'.gif')\n",
    "#     display(Image('reconstructed_video_'+str(i)+'.gif'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the Phenaki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not ('cvivit' in locals()):\n",
    "#     cvivit = CViViT(\n",
    "#         dim = 512,\n",
    "#         codebook_size = 65536,\n",
    "#         image_size =  (256, 256),  # video with rectangular screen allowed\n",
    "#         patch_size = 32,\n",
    "#         temporal_patch_size = 2,\n",
    "#         spatial_depth = 4,\n",
    "#         temporal_depth = 4,\n",
    "#         dim_head = 64,\n",
    "#         heads = 8\n",
    "#     )\n",
    "#     cvivit.load('./results/vae.2600.pt')\n",
    "\n",
    "# maskgit = MaskGit(\n",
    "#                 dim=cvivit.dim,\n",
    "#                 num_tokens=cvivit.codebook_size,\n",
    "#                 max_seq_len = 1024,\n",
    "#                 dim_context = 768,\n",
    "#                 depth = 6\n",
    "#             )\n",
    "# phenaki = Phenaki(\n",
    "#     cvivit = cvivit,\n",
    "#     maskgit = maskgit,\n",
    "#     self_token_critic= True  # set this to True\n",
    "# ).to(device)\n",
    "# phenaki_trainer = PhenakiTrainer(\n",
    "#     phenaki,\n",
    "#     batch_size=4,\n",
    "#     num_frames=17,\n",
    "#     train_lr=0.0001,\n",
    "#     train_num_steps=2,\n",
    "#     grad_accum_every = 2,\n",
    "#     train_on_images=False,\n",
    "#     save_and_sample_every=100,\n",
    "#     num_samples=4,\n",
    "#     dataset = train_dataset,\n",
    "#     sample_texts_file_path = f\"{'/content' if IN_COLAB else '/home/luthando/Desktop'}/phenaki-pytorch/data/sample_texts.txt\" # each caption should be on a new line, during sampling, will be randomly drawn\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phenaki_trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video = phenaki.sample(texts = 'a squirrel examines an acorn', num_frames = 17, cond_scale = 5.) # (1, 3, 17, 256, 128)\n",
    "\n",
    "# # so in the paper, they do not really achieve 2 minutes of coherent video\n",
    "# # at each new scene with new text conditioning, they condition on the previous K frames\n",
    "# # you can easily achieve this with this framework as so\n",
    "\n",
    "# video_prime = video[:, :, -3:] # (1, 3, 3, 256, 128) # say K = 3\n",
    "# video_next = phenaki.sample(texts = 'a cat watches the squirrel from afar', prime_frames = video_prime, num_frames = 14) # (1, 3, 14, 256, 128)\n",
    "\n",
    "# # the total video\n",
    "# entire_video = torch.cat((video, video_next), dim = 2) # (1, 3, 17 + 14, 256, 128)\n",
    "# entire_video.shape # (1, 3, 17 + 14 + 14 = 45, 256, 256)\n",
    "\n",
    "# # and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ... above code\n",
    "\n",
    "# from phenaki_pytorch import make_video\n",
    "\n",
    "# entire_video, scenes = make_video(phenaki, texts = [\n",
    "#     'a squirrel examines an acorn buried in the snow',\n",
    "#     'a cat watches the squirrel from a frosted window sill',\n",
    "#     'zoom out to show the entire living room, with the cat residing by the window sill'\n",
    "# ], num_frames = (17, 14, 14), prime_lengths = (5, 5))\n",
    "\n",
    "# entire_video.shape # (1, 3, 17 + 14 + 14 = 45, 256, 256)\n",
    "\n",
    "# # scenes - List[Tensor[3]] - video segment of each scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # video = entire_video[0].permute(1, 2, 3, 0) # CTHW -> THWC\n",
    "# # video = video.cpu().numpy()#.astype('uint8')\n",
    "\n",
    "# # # fig = plt.figure()\n",
    "# # fig = plt.figure(figsize=(2.2,2.2))  #Display size specification\n",
    "# # fig.subplots_adjust(left=0, right=1, bottom=0, top=1)\n",
    "# # plt.axis('off')\n",
    "# # im = plt.imshow(video[0, :, :, :])\n",
    "# # plt.close()\n",
    "# # def init():\n",
    "# #     im.set_data(video[0, :, :, :])\n",
    "# # def animate(i):\n",
    "# #     im.set_data(video[i, :, :, :])\n",
    "# #     return im\n",
    "# # anim = animation.FuncAnimation(fig, animate, init_func=init, frames=video.shape[0], interval=200) # 200ms = 5 fps\n",
    "# # display(HTML(anim.to_html5_video()))\n",
    "\n",
    "# from phenaki_pytorch.data import video_tensor_to_gif\n",
    "# from IPython.display import display, Image\n",
    "\n",
    "# video = entire_video[0]#.permute(1, 2, 3, 0) # CTHW -> THWC\n",
    "# video = video.cpu()#.numpy()#.astype('uint8')\n",
    "# print('generated video:')\n",
    "# video_tensor_to_gif(video, 'generated_video_.gif')\n",
    "# display(Image('generated_video_.gif'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
